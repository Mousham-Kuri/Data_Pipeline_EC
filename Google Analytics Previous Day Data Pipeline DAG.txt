from airflow.operators.python import PythonOperator
from airflow.models import DAG
from airflow.models import Variable
from datetime import datetime, timedelta
from google.analytics.data_v1beta import BetaAnalyticsDataClient
from google.analytics.data_v1beta.types import DateRange
from google.analytics.data_v1beta.types import Dimension
from google.analytics.data_v1beta.types import Metric
from google.analytics.data_v1beta.types import RunReportRequest
from google.cloud import storage
from google.cloud import bigquery
import pandas as pd
from airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator
from airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator
from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator
from airflow.hooks.base import BaseHook
from airflow.providers.slack.operators.slack_webhook import SlackWebhookOperator

SLACK_CONN_ID = 'slack'
dag_config = Variable.get("google_analytics_main_bigquery_table", deserialize_json=True)


def task_fail_slack_alert(context):
    slack_webhook_token = BaseHook.get_connection(SLACK_CONN_ID).password
    slack_msg = """
            :red_circle: Task Failed.
            *Task*: {task}  
            *Dag*: {dag} 
            *Execution Time*: {exec_date}  
            *Log Url*: {log_url} 
            """.format(
            task=context.get('task_instance').task_id,
            dag=context.get('task_instance').dag_id,
            ti=context.get('task_instance'),
            exec_date=context.get('data_interval_start'),
            log_url=context.get('task_instance').log_url,
        )
    failed_alert = SlackWebhookOperator(
        task_id='slack_test',
        http_conn_id=SLACK_CONN_ID,
        webhook_token=slack_webhook_token,
        message=slack_msg,
        channel=Variable.get("slack_channel"),
        username=Variable.get("slack_channel_username"),
        dag=dag)
    return failed_alert.execute(context=context)

def task_success_slack_alert(context):
    slack_webhook_token = BaseHook.get_connection(SLACK_CONN_ID).password
    slack_msg = """
            :large_green_circle: TASK SUCCESS!
            *Task*: {task} 
            *Dag*: {dag} 
            *Execution Time*: {exec_date}  
            *Log Url*: {log_url} 
            """.format(
            task=context.get('task_instance').task_id,
            dag=context.get('task_instance').dag_id,
            ti=context.get('task_instance'),
            exec_date=context.get('logical_date'),
            log_url=context.get('task_instance').log_url,
        )
    success_alert = SlackWebhookOperator(
        task_id='slack_test',
        http_conn_id='slack',
        webhook_token=slack_webhook_token,
        message=slack_msg,
        channel="de_airflow_data_jobs_dev",
        username='airflow',
        dag=dag)
    return success_alert.execute(context=context)

args = {
    'owner': 'airflow',
    'start_date': datetime(2022, 7, 31),
    'on_failure_callback': task_fail_slack_alert,
    'on_success_callback': task_success_slack_alert,
}

dag = DAG(
    dag_id='Google_Analytics_Dev_Previous_Day',
    default_args=args,
    schedule_interval='30 05 * * *',
    catchup=False,
    render_template_as_native_obj=True,
)

with dag:

    today = datetime.today().date()
    day_before_date = today - timedelta(days=4)
    day_before_str = str(day_before_date)
    day_before = day_before_str[:4] + day_before_str[5:7] + day_before_str[8:]

    def run_report_event(property_id):
        storage_client = storage.Client()
        bucket = storage_client.bucket("eclub_data_dev_ga")


        """Runs a simple report on a Google Analytics 4 property."""

        # Using a default constructor instructs the client to use the credentials
        # specified in GOOGLE_APPLICATION_CREDENTIALS environment variable.
        client = BetaAnalyticsDataClient()

        request = RunReportRequest(
            property=f"properties/{property_id}",
            dimensions=[Dimension(name="date"), Dimension(name="eventName"), Dimension(name="platform"),
                        Dimension(name="platformDeviceCategory"), Dimension(name="newVsReturning"),
                        Dimension(name="city")],
            metrics=[Metric(name="activeUsers"), Metric(name="newUsers"), Metric(name="userEngagementDuration"),
                     Metric(name="engagedSessions"), Metric(name="engagementRate"),
                     Metric(name="averageSessionDuration"), Metric(name="sessionsPerUser"), Metric(name="eventCount"),
                     Metric(name="eventCountPerUser")],
            date_ranges=[DateRange(start_date=day_before_str, end_date=day_before_str)], )
        response = client.run_report(request)

        output = []

        for row in response.rows:
            output.append({"date": row.dimension_values[0].value, "event_name": row.dimension_values[1].value,
                           "platform": row.dimension_values[2].value,
                           "platform_device_category": row.dimension_values[3].value,
                           "new_vs_returning": row.dimension_values[4].value, "city": row.dimension_values[5].value,
                           "active_users": row.metric_values[0].value, "new_users": row.metric_values[1].value,
                           "user_engagement_duration": row.metric_values[2].value,
                           "engaged_sessions": row.metric_values[3].value,
                           "engagement_rate": row.metric_values[4].value,
                           "average_session_duration": row.metric_values[5].value,
                           "sessions_per_user": row.metric_values[6].value, "event_count": row.metric_values[7].value,
                           "event_count_per_user": row.metric_values[8].value})
        df = pd.DataFrame(output)
        df[['date']] = df[['date']].applymap(str).applymap(lambda s: "{}-{}-{}".format(s[0:4], s[4:6], s[6:], ))
        file_name = "test1/google_analytics_" + str(day_before) + ".csv"
        csv_data = df.to_csv(index=False, header=True)
        blob = bucket.blob(file_name)
        blob.upload_from_string(csv_data, 'text/csv')
        print(str(file_name))


    google_analytics = PythonOperator(
        task_id='google_analytics',
        python_callable=run_report_event,
        op_kwargs={"property_id": 263944243},
    )


    def delete_from_bq(DATE):
        client_bq = bigquery.Client()
        query_string =""" 
                            delete from `eclub-data.dev.main_google_analytics ` where date=@date 
                            """
        job_config1 = bigquery.QueryJobConfig(
            query_parameters=[
                bigquery.ScalarQueryParameter("date", DATE, day_before_str),
            ]
        )
        client_bq.query(query_string, job_config=job_config1).result()

    delete_from_bq = PythonOperator(
        task_id='delete_from_bq',
        python_callable=delete_from_bq,
        op_kwargs={"DATE": "DATE"},
    )

    gcs_to_bigquery = GCSToBigQueryOperator(
        task_id='gcs_to_bigquery',
        bucket=Variable.get("bucket_google_analytics"),
        source_objects=Variable.get("google_analytics_source_object"),
        destination_project_dataset_table=Variable.get("google_analytics_stg_bigquery_table"),
        write_disposition='WRITE_TRUNCATE',
        skip_leading_rows=1,
        compression='GZIP',
        autodetect=True,
    )

    big_query_stg_to_main = BigQueryInsertJobOperator(
        task_id='big_query_stg_to_main',
        configuration={
            'query': {
                'query': Variable.get("google_analytics_select_query"),
                'destinationTable': {
                    'projectId': dag_config["projectId"],
                    'datasetId': dag_config["datasetId"],
                    'tableId': dag_config["tableId"],
                },
                'useLegacySql': False,
                'allowLargeResults': True,
                'write_disposition': 'WRITE_APPEND',
            }
        },
    )

    move_files = GCSToGCSOperator(
        task_id="move_files",
        source_bucket=Variable.get("bucket_google_analytics"),
        source_object=Variable.get("google_analytics_source_object"),
        destination_bucket=Variable.get("bucket_google_analytics"),
        destination_object=Variable.get("google_analytics_destination_object"),
        # delimiter='.csv',
        move_object=True,
        replace=True,
    )

    google_analytics >> delete_from_bq >> gcs_to_bigquery >> big_query_stg_to_main >> move_files